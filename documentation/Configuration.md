---
title: Porcupine configuration document
author: Yves ParÃ¨s
---

# Pipeline YAML configuration

## Configuration file sections

Every porcupine pipeline of tasks generates its own configuration file. This
file (YAML by default) can contain two sections: bindings and embedded data.

- The bindings (section `locations:`) associates virtual files (datasinks and
  datasources) to physical paths and locations.
- The embedded data (section `data:`) associates virtual fields directly to the
  data that the pipeline should read from them. Of course the config file being
  YAML or JSON, that requires that the data is representable in JSON

Both sections (`data:` and `locations:`) are actually two views of the same
resource tree, but `data:` describes an actuall data tree, while `locations:`
describe a _list_ of mappings. But each virtual path manipulated by the pipeline
can be in either one section or the other (or even in some cases, both). For
instance, this is example1 default config file, as generated by
`write-config-template`:

```yaml
variables: {}
data:
  Settings:
    users: 0
  Inputs: {}
  Outputs: {}
locations:
  /Inputs/User: _-{userId}.json
  /: porcupine-core/examples/data
  /Outputs/Analysis: _-{userId}.json
```

`Inputs` in `data:` and `/Inputs` in `locations:` is the same virtual folder. It
should not by default contain any embedded data, so that's why it is present
under `data:` but it just contains an empty object. Under `locations:`, it is
present in the path of `/Inputs/User` which itself is bound to a physical
location.

## Syntax of data section

The `data:` section is mostly dependent on the pipeline at hand. It will expose
the tree of resources (in the form of hierarchical JSON/YAML objects) used by
the pipeline. Each time your pipeline calls `getOption(s)`, or loads the data
from a virtual file created with `optionsVirtualFile` with a certain virtual
path, the default record of options provided will appear at this same path here,
in the configuration file generated by `write-config-template`.

But really any virtualfile from which we should read data deserializable from
JSON objects, arrays etc. can appear here. So even if a virtualfile isn't
present here by default, if you know that this virtualfile accepts JSON data,
you can add it in `data:` under the virtualfile's path, and map it to `null` in
the `locations:` section.

The only specific element of syntax to the `data:` section are the operators
starting with a `$` sign. For now there's just one, `$layers`, but we might
introduce more in the future.

### `$layers` operator

For instance, if we have a pipeline with the following task:

```haskell
import System.TaskPipeline
import qualified Data.HashMap as HM

type VarName = String
loadParams :: (LogThrow m) => PTask m () (HM.HashMap VarName Double)
loadParams = loadData $
  usesLayeredMapping $
    optionVirtualFile ["Inputs", "Parameters"]
      (docField @"bloodParams"
                (HM.fromList [("blood_pressure_sys", 120)
                             ,("blood_pressure_dia", 80)])
                "Map of blood-related parameters")
```

That generates the following configuration

```yaml
data:
  Inputs:
    Parameters:
      bloodParams:
        blood_pressure_sys: 120
        blood_pressure_dia: 80
```

`bloodParams:` is a fixed field. It's part of a record containing it as a unique
field (But we still need it because the CLI flags will be derived from that
field). However, the object under `bloodParams:` is read as a hashmap. The keys
present here are but default, and you could add more in the config. It's also
possible to spread these keys over several layers. This is what
`usesLayeredMapping` allows you to do, it registers that the hashmap we want to
read from configuration can be combined from several (any `Semigroup` can be
used here). For instance this is how you would do it:

```yaml
data:
  Inputs:
    Parameters:
      $layers:
        - bloodParams:
            hematocyte_count: 1000
        - bloodParams:
            blood_pressure_sys: 100
        - bloodParams:
            blood_pressure_sys: 120
            blood_pressure_dia: 80
```

In that case, the layers will be combined by `loadData` before returning the
result. The combination itself really depends on what type is being combined (in
other words, in which Semigroup instance is being used). For hashmaps, it works
by merging the keys, and if there's a key that conflicts (as
`blood_pressure_sys` does here), take the first one. This doesn't seem really
useful in that example (as we could have concatenated the maps yourself), but
has a real interest for when parts or all of your YAML config is generated
programmatically, and each layer comes from a different source of data. It is
especially useful if you have to implement more complex merge strategies than
just merging maps, or concatenating lists.


## Syntax of `locations:` section

This section is a simple JSON object, associating virtual paths to physical
paths. These paths might be accessed for reading or writing, but that
information isn't present in the configuration file. It is conventional that the
pipeline uses virtual paths beginning by a `/Inputs` directory for virtual files
that it wants to read, and `/Outputs` for virtual files that it wants to write,
but that is just a convention. Much like `data:`, the virtual paths present in
that section completely depend on the pipeline at hand.

### Types of mapping

#### Fully explicit mapping

The simplest form a mapping can take is this one:

```yaml
locations:
  /Inputs/User: my/path/to/the/file.json
```

here, `my/path/to/the/file.json` is understood as a path relative to the current
working directory in which the executable is started. Absolute paths work the
same. The pipeline is also told that the file to read will be a JSON file. This
will then result in an error if in that pipeline, `/Inputs/User` doesn't have a
JSON deserializer.

#### Mapping with implicit extension

```yaml
locations:
  /Outputs/Results: the_results
```

Here, `/Outputs/Results` is a virtual file that the pipeline will try to
write. We want the physical file in which we write to be `the_results` (so a
file in the working directory). We didn't specify the extension, but every
virtual file used in a pipeline has a default serializer. This is the one that
will be used here. So if the default serializer for `/Outputs/Results` is JSON,
then the pipeline will create the file `./the_results.json`.

#### Inherited mappings

This syntax is used to shorten the configuration for when you want to map a
virtual folder and let porcupine automatically derive default mappings for the
files it contains:

```yaml
locations:
  /: top_level_dir
  /Inputs/User: _.json
  /Outputs/Results: _.json
```

this is expanded by porcupine to the following:

```yaml
  /Inputs/User: top_level_dir/Inputs/User.json
  /Outputs/Results: top_level_dir/Outputs/Results.json
```

The default mapping generated by `write-config-template` maps the root to a
folder, and just uses inherited mapping for every virtual file of the
pipeline. This is by the way exactly the same than just writing:

```yaml
locations:
  /: top_level_dir
```

however `write-config-template` generates the more verbose default configuration
(listing all virtual files), because it's easier to modify some pipeline's
mappings if each virtual file manipulated by the pipeline is already mentioned
in the config file.


### Using layers

Just like in `data:`, we can ask (if the virtual file `usesLayeredMapping`) to
merge the content of several files when reading a virtual file:

```yaml
locations:
  /Inputs/User:
    - userFile1.json
    - userFile2.json
```

There is no `$layers` keyword here, just put the files as an array after the
virtual path. The files will be read, deserialized and merged all the same.

If a virtual file has **at the same time** embedded data associated to it under
`data:` and physical path(s) mapped to it under `location:`, then the embedded
data layers act as the first layers and the ones read from external files as the
last layers. It is as if they were all part of the same `$layers:` group under
`data:`. **If you don't want this to happen** when you embed data in the `data:`
section, don't forget to map to `null` the corresponding virtual path in
`locations:`. Is is **important** to remember that most virtual files are
implicitly mapped by default, so you need to explicitly map them to `null`.

### Using variables in locations

There are 2 kinds of variables: the ones defined purely by the config file, and
the ones used inside the pipeline to disambiguate several occurences of the
_same_ virtual files. Although the syntax to splice them in a physical path is
the same.

#### Custom variables

Your YAML config can contain a `variables:` section at the top level:

```yaml
variables:
  var1: 90
  var2: hello
locations:
  /Inputs/Stuff: stuff-{var1}.json
  /Outputs/OtherStuff: file-{var2}.json
```

Here, var1 and var2 in the paths will simply be replaced by the values
associated to them. So it is as if we wrote:

```yaml
locations:
  /Inputs/Stuff: stuff-90.json
  /Outputs/OtherStuff: file-hello.json
```

The point of it is that variables can be overriden on the command line with
`--var`, so you can quickly change the input file for instance in the example
above by running with `--var var1=91`. The values in the `variables:` section
are just default values, and they can even be missing from that section (but you
will get a runtime error if you forget to set a value with `--var` then).


#### Repeated virtual files

It often happens that a pipeline wants to read multiple input files, all
following the same schema, the number of which isn't known in advance.  A
pipeline can do so by using repeated virtual files. This often done with
`loadDataList`, `loadDataStream`, `writeDataList` and `writeDataStream`. Each of
these function takes an extra parameter: a variable name. Plus the task they
produce must be fed in input a sequence (list or stream) of values for that
variable, that will be spliced in to the final path that will be written. These
values are called _indices_. Often, that sequence of indices is obtained via an
option of the pipeline (so a call to `getOption(s)`). The `example1` default
config file for instance contains:

```yaml
locations:
  /Inputs/User: _-{userId}.json
  /Outputs/Analysis: _-{userId}.json
```

The presence of a `{userId}` variable in both the input and the output virtual
files means that both these virtual files are repeatedly read and written by the
pipeline, but each time with a different index. Therefore that index must be
spliced in the physical path, so that we don't end up reading or overwriting the
same file again and again. However that variable can be anywhere we want in the
path.

It also happens that tasks that expect to work on unique virtual files (no
variables) can be transformed to work on specific occurences of these virtual
files. This is what `parMapTask` does, to execute several instances of the same
task in parallel, each one on an different element of an input
list. `parMapTask` takes a variable name, like `loadDataList`, and will use that
variable to modify **every** virtual file used internally in the
task. `example1` does it to repeat the same task over different input files.


### Differences between layered mapping and repeated virtual files

One could argue that layered mapping and repetition of virtual files through an
index serve the same purpose: associate _several_ physical files to _one_
virtual file. It's true, but both serve different purposes.

You will use layers when a single data can be conveniently separated into a few
physical files (you wouldn't use layers when you have 100 files to list as
layers), each one adding extra data. You would also use layers when you want
some notion of _overriding_: depending on the Semigroup instance, a layer can
override the layers after/before it (like for hashmaps for instance). For
instance if you want to load from a file parameters for an experiment, you can
put the base parameters with their default values in a layers that will always
be used, and put more specific sets of values in other files, as layers that
will be swapped in and out as you need, to override the base layer. As the
physical files used as layers don't need to be placed one next to another (or to
be linked in any way), that leaves you with total flexibility as to how you want
to organize your experiments' configurations. Using different location
accessors, the base layer can ever be read from one source of data while the
override layers can be read from local files.

Repeated virtual files are stricter than layers: you cannot map two arbitrarily
different paths as two occurences of the same virtual file, _they have to be the
same path up to some variable_. But this comes with the advantage that repeated
virtual files can exist in any amount you want. The prefered method to handle
them is to use `loadDataStream`/`writeDataStream`, as the streams are never
fully loaded in memory. Therefore they allow your pipeline to run in constant
memory whatever is the amount of files it needs to read (`load`/`writeDataList`
are just convenience tasks for when you need all the data to be present in
memory at the same time anyway).
